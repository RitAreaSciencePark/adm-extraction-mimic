{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48fa7709-50a0-4ee5-9eb6-91964d55ce1d",
   "metadata": {},
   "source": [
    "# Admission extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5031a27d-f3f8-4d9c-acd1-b71c231ee05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 15:42:00 [__init__.py:243] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field, validator\n",
    "\n",
    "import os\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Use vLLM, should be faster than pipeline\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# To download the models\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "550eaf90-04a8-4119-95f4-8fab9f6eda9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze -l > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b7b9f-ad38-4d57-a219-9e91d419763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = # add hugginface api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72de58c8-b784-4a74-bcb5-c7c157c01803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mimic-iv/physionet/hosp/mimiciv_hosp.db',\n",
       " 'mimic-iv/physionet/files/mimiciv/2.2')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MIMIC_FILES_BASE_FOLDER: os.path = os.path.join(\"\", \"mimic-iv\", \"physionet\", \"files\", \"mimiciv\", \"2.2\") # mimic base folder\n",
    "\n",
    "MIMIC_FILES_BASE_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a3958d-cb73-49e4-93d9-225b6d610a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/orfeo/cephfs/scratch/area/egoat/mimic-iv/physionet/files/mimiciv/2.2/hosp/discharge.csv.gz'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NOTES_PATH: os.path = os.path.join(os.getcwd(), MIMIC_FILES_BASE_FOLDER, 'hosp', 'discharge.csv.gz') # discharge file\n",
    "NOTES_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f46c16-48e2-4dfa-9df6-922b4dc2d2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iter(path: os.path, chunksize: int = 100000): # since file is to large to load directly as a pandas df load it as chuncks\n",
    "    '''\n",
    "    This function is used to read the file and get a sort of iterator on the .csv file\n",
    "    '''\n",
    "    return pd.read_csv(path, compression='gzip', chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db3c132-636f-4c66-93cd-e571c9aa1ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdmissionPhysicalExam(BaseModel):\n",
    "    \"\"\"Represents the findings of an admission physical examination across various systems.\"\"\"\n",
    "\n",
    "    heent: Optional[str] = Field(\n",
    "        default=None, description=\"PE Section: Head, Eyes, Ears, Nose, Throat Exam or HEENT\"\n",
    "    )\n",
    "    neuro: Optional[str] = Field(\n",
    "        default=None, description=\"PE Section: Neurological Exam\"\n",
    "    )\n",
    "    vs: Optional[str] = Field(\n",
    "        default=None, description=\"PE Section: Vital Signs or Vitals\"\n",
    "    )\n",
    "    general: Optional[str] = Field(\n",
    "        default=None, description=\"PE Section: General Appearance Exam\"\n",
    "    )\n",
    "    neck: Optional[str] = Field(\n",
    "        default=None, description=\"PE Section: Neck Exam\"\n",
    "    )\n",
    "    skin: Optional[str] = Field(\n",
    "        default=None, description=\"PE Section: Skin Exam\"\n",
    "    )\n",
    "    lymph: Optional[str] = Field(\n",
    "        default=None, description=\"PE Section: Lymphatic System Exam\"\n",
    "    )\n",
    "    ext: Optional[str] = Field(\n",
    "        default=None, description=\"PE Section: Extremities Exam\"\n",
    "    )\n",
    "    abd: Optional[str] = Field(\n",
    "        default=None, description=\"PE Section: Abdominal Exam\"\n",
    "    )\n",
    "    psych: Optional[str] = Field(\n",
    "        default=None, description=\"PE Section: Psychiatric Exam\"\n",
    "    )\n",
    "    cv: Optional[str] = Field(\n",
    "        default=None, description=\"PE Section: Cardiovascular Exam\"\n",
    "    )\n",
    "    resp: Optional[str] = Field(\n",
    "        default=None, description=\"PE Section: Respiratory Exam\"\n",
    "    )\n",
    "    ent: Optional[str] = Field(\n",
    "        default=None, description=\"PE Section: Ears, Nose, and Throat Exam\"\n",
    "    )\n",
    "    back: Optional[str] = Field(\n",
    "        default=None, description=\"PE Section: Back Exam\"\n",
    "    )\n",
    "    chest: Optional[str] = Field(\n",
    "        default=None, description=\"PE Section: Chest Exam\"\n",
    "    )\n",
    "    gu: Optional[str] = Field(\n",
    "        default=None, description=\"PE Section: Genitourinary Exam\"\n",
    "    )\n",
    "    spine: Optional[str] = Field(\n",
    "        default=None, description=\"PE Section: Spine Exam\"\n",
    "    )\n",
    "    head: Optional[str] = Field(\n",
    "        default=None, description=\"PE Section: Head Exam\"\n",
    "    )\n",
    "    gi: Optional[str] = Field(\n",
    "        default=None, description=\"PE Section: Gastrointestinal Exam\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae705b9-12c5-4677-8453-510877e7f6b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !!!! Download the whole medGemma model to a local folder !!!\n",
    "# local_dir = snapshot_download(repo_id=\"google/medgemma-27b-text-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3397629-0c29-45b1-b5f8-a0dcb02ff3a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 15:42:02 [__init__.py:31] Available plugins for group vllm.general_plugins:\n",
      "INFO 06-03 15:42:02 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\n",
      "INFO 06-03 15:42:02 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\n",
      "INFO 06-03 15:42:13 [config.py:793] This model supports multiple tasks: {'score', 'embed', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 06-03 15:42:13 [config.py:1875] Defaulting to use mp for distributed inference\n",
      "INFO 06-03 15:42:13 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 06-03 15:42:13 [cuda.py:87] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 06-03 15:42:17 [core.py:438] Waiting for init message from front-end.\n",
      "INFO 06-03 15:42:17 [core.py:65] Initializing a V1 LLM engine (v0.9.0.1) with config: model='google/medgemma-27b-text-it', speculative_config=None, tokenizer='google/medgemma-27b-text-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=10000, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=google/medgemma-27b-text-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={\"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"cudagraph_capture_sizes\": [], \"max_capture_size\": 0}\n",
      "WARNING 06-03 15:42:17 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 8 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 06-03 15:42:17 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 10485760, 10, 'psm_ac10760a'), local_subscribe_addr='ipc:///tmp/3c82e87d-de78-4d59-b533-5d7efcc3d2a7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 06-03 15:42:17 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7faa56cef7c0>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2887147)\u001b[0;0m INFO 06-03 15:42:17 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_6d4a79bb'), local_subscribe_addr='ipc:///tmp/8101e1f2-168a-4133-bd03-42e074643c19', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "WARNING 06-03 15:42:17 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7faa56cee080>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2887148)\u001b[0;0m INFO 06-03 15:42:17 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8523d1ac'), local_subscribe_addr='ipc:///tmp/278c3f9d-70e7-4536-9553-afda93316495', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2887148)\u001b[0;0m INFO 06-03 15:42:19 [utils.py:1077] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2887147)\u001b[0;0m INFO 06-03 15:42:19 [utils.py:1077] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2887148)\u001b[0;0m INFO 06-03 15:42:19 [pynccl.py:69] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2887147)\u001b[0;0m INFO 06-03 15:42:19 [pynccl.py:69] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2887147)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=2887148)\u001b[0;0m INFO 06-03 15:42:19 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /u/area/egoat/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 06-03 15:42:19 [custom_all_reduce_utils.py:245] reading GPU P2P access cache from /u/area/egoat/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2887147)\u001b[0;0m INFO 06-03 15:42:20 [shm_broadcast.py:250] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_e7330cd1'), local_subscribe_addr='ipc:///tmp/882cd6b9-c3be-42a6-91e4-f743ac5faa89', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2887148)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=2887147)\u001b[0;0m INFO 06-03 15:42:20 [parallel_state.py:1064] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 06-03 15:42:20 [parallel_state.py:1064] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2887148)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=2887147)\u001b[0;0m WARNING 06-03 15:42:20 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 06-03 15:42:20 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2887148)\u001b[0;0m INFO 06-03 15:42:20 [gpu_model_runner.py:1531] Starting to load model google/medgemma-27b-text-it...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2887147)\u001b[0;0m INFO 06-03 15:42:20 [gpu_model_runner.py:1531] Starting to load model google/medgemma-27b-text-it...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2887148)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=2887147)\u001b[0;0m INFO 06-03 15:42:20 [cuda.py:217] Using Flash Attention backend on V1 engine.\n",
      "INFO 06-03 15:42:20 [cuda.py:217] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2887148)\u001b[0;0m INFO 06-03 15:42:20 [weight_utils.py:291] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2887147)\u001b[0;0m INFO 06-03 15:42:20 [weight_utils.py:291] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bcadf3a1aa94d43a949d44f2af54bd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/11 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=2887148)\u001b[0;0m INFO 06-03 15:42:42 [default_loader.py:280] Loading weights took 21.96 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2887148)\u001b[0;0m INFO 06-03 15:42:43 [gpu_model_runner.py:1549] Model loading took 25.4906 GiB and 22.849613 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2887147)\u001b[0;0m INFO 06-03 15:42:43 [default_loader.py:280] Loading weights took 22.08 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2887147)\u001b[0;0m INFO 06-03 15:42:43 [gpu_model_runner.py:1549] Model loading took 25.4906 GiB and 23.683553 seconds\n",
      "INFO 06-03 15:42:50 [kv_cache_utils.py:637] GPU KV cache size: 29,264 tokens\n",
      "INFO 06-03 15:42:50 [kv_cache_utils.py:640] Maximum concurrency for 10,000 tokens per request: 2.93x\n",
      "INFO 06-03 15:42:50 [kv_cache_utils.py:637] GPU KV cache size: 29,264 tokens\n",
      "INFO 06-03 15:42:50 [kv_cache_utils.py:640] Maximum concurrency for 10,000 tokens per request: 2.93x\n",
      "INFO 06-03 15:42:52 [core.py:167] init engine (profile, create kv cache, warmup model) took 8.01 seconds\n"
     ]
    }
   ],
   "source": [
    "model_id = \"google/medgemma-27b-text-it\" # \"google/medgemma-27b-text-it\" or \"google/medgemma-4b-it\"\n",
    "\n",
    "llm = LLM(model=model_id,\n",
    "          dtype=torch.bfloat16, # bfloat16 \n",
    "          tensor_parallel_size=2, # enable multi-gpu\n",
    "          max_model_len=1e4,   # max contex window\n",
    "          max_num_seqs=10,       # Limit batch size,\n",
    "          enforce_eager=True # disable cuda graph to reduce memory but it decreses perfomances\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.0,  # This enables greedy decoding\n",
    "    max_tokens=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7d96f3c-2ad5-448f-ad6f-59d87d7093ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = r\"\"\"\n",
    "You are a dataâ€extraction assistant.\n",
    "\n",
    "Task:\n",
    "1. From the provided clinical note, locate the \"Physical Exam\" section.\n",
    "2. Within that section, extract **only** the text under the **Admission** subsection.\n",
    "\n",
    "## Critical Extraction Rules:\n",
    "- **START ONLY** at \"Admission:\" heading - ignore ALL content before this heading\n",
    "- **EXCLUDE** any preceding content  \n",
    "- **STOP** extraction at next heading (e.g., \"Findings:\", \"Impression:\", \"Assessment:\") or clear section break\n",
    "- **WHEN VERY UNCERTAIN** about text boundaries: **EXCLUDE rather than include**\n",
    "- **PRESERVE** multi-line sentences within the Admission subsection\n",
    "\n",
    "The extracted text data **must** be outputted in the following format:\n",
    "**{format_instructions}**\n",
    "\n",
    "Text:\n",
    "{text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3305918-01f5-4920-924d-15e54ca0adec",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=AdmissionPhysicalExam) # pydantic parser for AdmissionPhysicalExam\n",
    "\n",
    "creation_prompt = PromptTemplate(\n",
    "    template=template,    input_variables=[\"text\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ") # Creation of the template prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6ab378-965a-478b-9bbb-29901ddae7a7",
   "metadata": {},
   "source": [
    "# True Extraction of the first 100 discharges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234c40f0-5f82-4f6f-8383-adfcf55828fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE: int = 2\n",
    "\n",
    "random_discharge_batch: pd.DataFrame = next(iter(get_iter(NOTES_PATH))).iloc[:100]\n",
    "\n",
    "responces: List[AdmissionPhysicalExam] = []\n",
    "\n",
    "for idx in range(0, 100, BATCH_SIZE):\n",
    "    cbatch: pd.DataFrame = random_discharge_batch['text'].iloc[idx: idx + BATCH_SIZE] # loading a batch of two patients\n",
    "    messages_batch = [ # creating an list of messagases that have system prompt and a user prompt\n",
    "        [{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful medical assistant.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": creation_prompt.format(text=text)\n",
    "        }] for text in cbatch\n",
    "    ]\n",
    "\n",
    "    outputs = llm.chat(messages_batch, sampling_params=sampling_params, use_tqdm=True, add_generation_prompt=True, continue_final_message=False)\n",
    "\n",
    "    for out_data in outputs:\n",
    "        responces.append(parser.parse(out_data.outputs[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5a4099",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(responces)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
